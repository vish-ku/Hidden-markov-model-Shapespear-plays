{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hidden Markov Model- Shakespear play.",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoU7dp3nnX9B"
      },
      "source": [
        "## Loading all the libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibggJABQV8yB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqKbu4rpnfat"
      },
      "source": [
        "## Loading and cleaning the data.\n",
        "We take the words from Shakespears play, Hamlet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4CreyanWXfT"
      },
      "source": [
        "# loading the data\n",
        "data = pd.read_csv('sample_data/Shakespeare_data.csv')\n",
        "\n",
        "# getting rid of unwanted rows.\n",
        "df = data.dropna (subset = ['Player', 'ActSceneLine'])\n",
        "\n",
        "# Getting the words from the play Hamlet.\n",
        "new_data = df.loc[data['Play'] == 'Hamlet',['PlayerLine']]\n",
        "lines = new_data['PlayerLine'].values.tolist()\n",
        "\n",
        "# removing all the punctuations. Excluding 'period'\n",
        "punc = '''!()-[]{};:'\"\\,<>/?@#$%^&*_~\"\"'''\n",
        "for i in range(len(lines)):\n",
        "    temp_line = \"\"\n",
        "    for item in lines[i]:\n",
        "        if item not in punc:\n",
        "            temp_line += item\n",
        "    \n",
        "    lines[i] = temp_line\n",
        "\n",
        "# forming a list of words.\n",
        "words = \"\"\n",
        "for i in range(len(lines)):\n",
        "    words = words + lines[i].lower()+ \" \"        \n",
        "words = words.split(\" \")\n",
        "\n",
        "# collecting all the unique words in the play Hamlet!\n",
        "unique = []\n",
        "for item in words:\n",
        "    if item not in unique:\n",
        "        unique.append(item)\n",
        "unique.remove(\"\")\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpoZIzj4nsrH"
      },
      "source": [
        "## Finding the parameters of Hidden Markov Model using the Baum Welch algorithm.\n",
        "\n",
        "The parameters are being saved in a pickle file named parameters.p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlAaq5KLMlM1",
        "outputId": "8818630b-0696-4f60-9fe3-c2ec0bc7749c"
      },
      "source": [
        "# We will use the Baum-welch algorithm! This is a lot similar to Expectation Maximization algorithm done in the first project.\n",
        "# The algorithms have been referred and understood from the textbook: \n",
        "# Also to avoid many underflow errors a scaled version of the parameters are used. \n",
        "\n",
        "def normalized(a):\n",
        "    return a/sum(a)\n",
        "\n",
        "\n",
        "def train(words, latent_variables):\n",
        "    n = len(words)\n",
        "    # start time\n",
        "    begin = time.time()\n",
        "\n",
        "    # To train the model, we need to initiale the hidden markov model with initial parameters. \n",
        "    # We train the model to update the parameters to get optimal ones. \n",
        "    \n",
        "    # We start the algorithm with initial parameters. \n",
        "    temp1 = np.random.uniform(10,50,latent_variables)\n",
        "    initial = normalized(temp1)\n",
        "    old_initial = np.empty([latent_variables])\n",
        "    \n",
        "    # Initial transmission probabilities. This is again done with Dirichlet distribution.\n",
        "    # see this as from rows to columns\n",
        "    ini_trans = np.empty([latent_variables,latent_variables])\n",
        "    for i in range(latent_variables):\n",
        "        temp2 = np.random.uniform(1,10,latent_variables)\n",
        "        ini_trans[i] = normalized(temp2)\n",
        "    old_ini_trans = np.empty([latent_variables,latent_variables])\n",
        "\n",
        "    # Initial emmision probabilities.\n",
        "    ini_emi = np.empty([latent_variables,n])\n",
        "    old_ini_emi = np.empty([latent_variables,n])\n",
        "\n",
        "    for i in range(latent_variables):\n",
        "        temp3 = np.random.uniform(1,6,n)\n",
        "        ini_emi[i] = normalized(temp3)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    while not np.array_equal(old_initial, initial):\n",
        "\n",
        "        if count > 150:\n",
        "            break\n",
        "\n",
        "        count +=1\n",
        "        #print(\"Iteration:\", count)\n",
        "\n",
        "        # For the algorithm to run we find forward parameters of the algorithm, backward part, gamma and etc..\n",
        "\n",
        "        # Finding scaled alpha, this is the forward part of the algorithm.\n",
        "        alpha = np.empty([n,latent_variables])\n",
        "        normalized_alpha = np.empty([n,latent_variables])\n",
        "        c = np.empty([n])\n",
        "        for i in range(latent_variables):\n",
        "            alpha[0][i] = initial[i]*ini_emi[i][0]\n",
        "\n",
        "        c[0] = sum(alpha[0])\n",
        "        if math.isnan(c[0]) == True:\n",
        "            print(\"Try again with new initial parameters\")\n",
        "            break\n",
        "        #print(c[0])\n",
        "        normalized_alpha[0] = normalized(alpha[0])\n",
        "\n",
        "        # Finding scaled alpha recursively. \n",
        "        for i in range(1,n):\n",
        "            temp4 = np.empty([latent_variables])\n",
        "            for j in range(latent_variables):\n",
        "                summ = 0\n",
        "                for z in range(latent_variables):\n",
        "                    summ += normalized_alpha[i-1][z] * ini_trans[z][j]\n",
        "                \n",
        "                temp4[j] = ini_emi[j][i] * summ\n",
        "\n",
        "            c[i] = sum(temp4)\n",
        "            normalized_alpha[i] = temp4/c[i]\n",
        "\n",
        "        \n",
        "        # Finding backward parameters\n",
        "        beta = np.empty([n,latent_variables])\n",
        "        normalized_beta = np.empty([n,latent_variables])\n",
        "        # initializing.. \n",
        "        beta[n-1] = np.ones([latent_variables])\n",
        "        normalized_beta[n-1] = np.ones([latent_variables])\n",
        "        for i in range(1,n):\n",
        "            temp = np.empty([latent_variables])\n",
        "            for j in range(latent_variables):\n",
        "                summ1 = 0\n",
        "                for z in range(latent_variables):\n",
        "                    summ1 += normalized_beta[n-i][z]*ini_emi[z][n-i]*ini_trans[j][z]\n",
        "\n",
        "                temp[j] = summ1\n",
        "            \n",
        "            normalized_beta[n-1-i] = temp/c[n-i]\n",
        "\n",
        "\n",
        "        # Gamma parameter in algorithm\n",
        "        gamma = np.empty([n,latent_variables])\n",
        "        for i in range(n):\n",
        "            for j in range(latent_variables):\n",
        "                gamma[i][j] = normalized_alpha[i][j] * normalized_beta[i][j]\n",
        "\n",
        "\n",
        "        # Xi parameter in algorithm\n",
        "        xi = np.empty([n,latent_variables,latent_variables])\n",
        "\n",
        "        for i in range(1,n):\n",
        "            for j in range(latent_variables):  # for z_n-1 variable\n",
        "                for z in range(latent_variables): # for z_n variable\n",
        "                    xi[i][j][z] = c[i]*normalized_alpha[i-1][j] * ini_emi[z][i] *ini_trans[j][z] * normalized_beta[i][z]\n",
        "\n",
        "\n",
        "        # Saving old parameters!\n",
        "        old_initial = np.copy(initial)\n",
        "        old_ini_trans = np.copy(ini_trans)\n",
        "        old_ini_emi = np.copy(ini_emi)\n",
        "\n",
        "        # Now we will use these parameter to update the initial parameters!\n",
        "\n",
        "\n",
        "        # updated initial latent variable probabilities. \n",
        "        for i in range(latent_variables):\n",
        "            initial[i] = gamma[0][i]/sum(gamma[0])\n",
        "        \n",
        "        #print(\" Initial probability: \",sum(initial))\n",
        "\n",
        "\n",
        "        # updated transition probabilities. \n",
        "        for i in range(latent_variables):\n",
        "            deno = 0\n",
        "            for l in range(latent_variables):\n",
        "                for k in range(1,n):\n",
        "                    deno += xi[k][i][l]\n",
        "            \n",
        "            for j in range(latent_variables):\n",
        "                nume = 0\n",
        "                for z in range(1,n):\n",
        "                    nume += xi[z][i][j]\n",
        "                \n",
        "                ini_trans[i][j] = nume/deno\n",
        "\n",
        "            #print(\"probability of transmission: \", sum(ini_trans[i]))\n",
        "\n",
        "\n",
        "        # updating emission prbabilities.\n",
        "        for i in range(latent_variables):\n",
        "            denom = sum(gamma[:,i])\n",
        "            for j in range(n):\n",
        "                ini_emi[i][j] = gamma[j][i]/denom\n",
        "\n",
        "        #print(np.array_equal(old_initial, initial))\n",
        "    # calculating end time.\n",
        "    end = time.time()\n",
        "\n",
        "    if count < 100:\n",
        "        print(\"Convergence of parameters achieved in \", count, \"iterations and in time: \", end-begin, \"seconds\")            \n",
        "    parameters = (initial, ini_trans, ini_emi,latent_variables)\n",
        "    \n",
        "    # Pickle helps save the model in a seperate file that can be accessed later. \n",
        "    pickle.dump(parameters, open('sample_data/pickle/parameters.p', 'wb'))\n",
        "        \n",
        "# training with 7 latent variables. One can think of them as different parts of speech in english, like noun, verb etc\n",
        "train(unique,7)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Convergence of parameters achieved in  13 iterations and in time:  29.666426420211792 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYMmdCDOoIfh"
      },
      "source": [
        "## The text generation and text prediction functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXKY-u7F19rS"
      },
      "source": [
        "# The word generating function! uses the trained model.\n",
        "# paramaters for both functions are the list of unique words in the play.\n",
        "\n",
        "def generate(words):\n",
        "    # loading the trained data\n",
        "    parameters = pickle.load(open('sample_data/pickle/parameters.p', 'rb'))\n",
        "\n",
        "    lat_var = parameters[3]\n",
        "    initial = parameters[0]\n",
        "    ini_trans = parameters[1]\n",
        "    ini_emi = parameters[2]\n",
        "\n",
        "    n = int(input(\"Enter number of words to generate: \"))\n",
        "    \n",
        "    # choosing a random initial latent variable in accordance with initial probabilities\n",
        "    latent = np.random.choice(list(range(lat_var)),p = initial)\n",
        "\n",
        "    sentence = \"\"\n",
        "    for i in range(n):\n",
        "        # chosing random word from latent variable in accordance with model emission parameters.\n",
        "        word = np.random.choice(words, p = ini_emi[latent])\n",
        "        if sentence == \"\":\n",
        "            word = word.capitalize()\n",
        "        elif sentence[-2] == \".\":\n",
        "            word = word.capitalize()\n",
        "        sentence += word + \" \"\n",
        "        # choosing next latent variable in accordance with model transision probabilities.\n",
        "        latent = np.random.choice(list(range(lat_var)), p = ini_trans[latent])\n",
        "\n",
        "    print(sentence)\n",
        "\n",
        "\n",
        "\n",
        "# This is the text prediction function. uses viterbi algorithm. \n",
        "# for the input, we have to use words in the argument of the function. \n",
        "\n",
        "def text_prediction(words):\n",
        "\n",
        "    # loading the trained data\n",
        "    parameters = pickle.load(open('sample_data/pickle/parameters.p', 'rb'))\n",
        "\n",
        "    no_late_var = parameters[3]\n",
        "    initial = parameters[0]\n",
        "    ini_trans = parameters[1]\n",
        "    ini_emi = parameters[2]\n",
        "\n",
        "\n",
        "    sentence = input(\"Write a sentence: \")\n",
        "    #sentence = \"whos there nay answer and long live king quiet guard\"\n",
        "    n = int(input(\"Enter no of words to predict: \"))\n",
        "\n",
        "    # spliting the sentence into a list of words\n",
        "    sentence_lst = sentence.split(\" \")\n",
        "\n",
        "    probabilities = np.empty([no_late_var, len(sentence_lst)])\n",
        "\n",
        "    x = np.empty([len(sentence_lst)])\n",
        "\n",
        "    # Figuring out best possible latent variable for the first word. \n",
        "    for i in range(no_late_var):\n",
        "        probabilities[i][0] = initial[i] * ini_emi[i][uniq.index(sentence_lst[0])]\n",
        "    # x stores the best latent variable!\n",
        "    x[0] = np.argmax(probabilities[:,0])\n",
        "    \n",
        "    #Figuring out best subsequent sequence of latent variables. (We find the highest probable latent variable at each step)\n",
        "    for z in range(1,len(sentence_lst)):\n",
        "        for i in range(no_late_var):\n",
        "            temp = np.empty([no_late_var])\n",
        "            for j in range(no_late_var):\n",
        "                temp[j] = probabilities[j][z-1] * ini_trans[j][i] * ini_emi[i][uniq.index(sentence_lst[z])]\n",
        "\n",
        "            index = np.argmax(temp)\n",
        "            \n",
        "            probabilities[i][z] = temp[index]\n",
        "        # Record the best possible latent variable\n",
        "        x[z] = np.argmax(probabilities[:,z])\n",
        "\n",
        "\n",
        "    #The last latent variable is x[-1], from here we follow the code for the generate function. \n",
        "    latent = int(x[-1])\n",
        "    predict = \" \"\n",
        "    for i in range(n):\n",
        "        word = np.random.choice(words, p = ini_emi[latent])\n",
        "        if i >0 and predict[-2] == \".\":\n",
        "            word = word.capitalize()\n",
        "        predict += word + \" \"\n",
        "        latent = np.random.choice(list(range(no_late_var)), p = ini_trans[latent])\n",
        "\n",
        "    print(\"Most probable words to follow are: \",predict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUdSQ_Dpr3dx"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzv9qFXh6Krb",
        "outputId": "441bfc13-38d7-4b82-eaf6-fd2fb0bc8aaf"
      },
      "source": [
        "generate(unique)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of words to generate: 50\n",
            "Too canker singeing smells gis dew plautus sterling. Earth. Selfslaughter rheum beaten littlest relish mouth forgone wag provincial natures. Forbid distrust beneath. Residence distempered. Dearly cellarage liferendering spring scarce courteous sigh loses warlike laertes purpose extremity. Rhymed. Roses carters. Entertainment womens attended. Melancholy stayd roses rheum eight pleasing charity normandy \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PErTn-77sE7d",
        "outputId": "bc91619a-803b-44cf-a14b-49aeaa04f3f0"
      },
      "source": [
        "generate(unique)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of words to generate: 20\n",
            "Craft impious unyoke. Afflict seat sick easy tediousness went ungracious attractive. Doors knavish acres orbed sliver necessary through distracted unmannerly. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoEWuksRtsbZ",
        "outputId": "6bf08075-bd9c-4285-82b3-59bd525b26ee"
      },
      "source": [
        "generate(unique)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of words to generate: 15\n",
            "Laertes. Hilts unite adoption certainty breaking way. Brainish twice wars fixd natural canon laertes tender \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUU6LlWeodLW",
        "outputId": "023451c2-483d-4edc-c9b5-4d18c3bd1161"
      },
      "source": [
        "text_prediction(unique)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write a sentence: long live the\n",
            "Enter no of words to predict: 5\n",
            "Most probable words to follow are:   prevent disasters crafts deprived physic \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky_AtKh-sWmk",
        "outputId": "c48fb933-1b28-427f-9450-2cf8d66470cc"
      },
      "source": [
        "text_prediction(unique)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write a sentence: i am going back to\n",
            "Enter no of words to predict: 4\n",
            "Most probable words to follow are:   went erring hundred adventurous \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMk_xmdNtzNT",
        "outputId": "6dc719f6-3e2c-420d-8d11-62f9ea212d0d"
      },
      "source": [
        "text_prediction(unique)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write a sentence: stars in the night will\n",
            "Enter no of words to predict: 4\n",
            "Most probable words to follow are:   satisfied plunge primal earth \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}